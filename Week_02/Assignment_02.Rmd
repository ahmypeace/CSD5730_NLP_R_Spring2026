---
title: "Assignment_02: Text Cleaning"
date: "2026-01-23"
output: 
  html_document:
    css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## _Now for your challenge!_
### Task Overview

In this assignment, you will work with a raw text file to practice basic text cleaning and tokenization in R. Specifically, you will:

- Read sticks_messy.txt into R from the data/ folder

- Create a homemade text cleaning function using at least three chained procedures

- Clean the raw text using your custom function

- Split the cleaned text into a one-word-per-row format

- Report the total number of tokens before and after cleaning (using " " as the separator)

- Compute the Type-Token Ratio (TTR) for both versions of the text

### Additional Requirements

Your workflow should include:

- Token splitting and counting steps

- Clear documentation of each cleaning step

- Chaining of cleaning procedures into a single function

- (Optional, for readability) A display of tokens in a three-words-per-row format

All code must be annotated, and the final submission should be a knitted HTML document, styled using your own CSS.


```{r}
## Import the file
# setting the file path
path <- file.path("data/sticks_messy.txt")

# read the file line by line
text_lines <- readLines(path, warn = FALSE, encoding = "UTF-8")

# combine all lines to one long string 
raw_text <- paste(text_lines, collapse = " ")

# show the first 300 character so  know that this is working
substr(raw_text, 1, 300)


## Step 2: Tokenize and count words before cleaning

# Split the text with " " wherever there is a space
tokens_before <- strsplit(raw_text, " ", fixed = TRUE)

# Convert list to vector
tokens_before <- unlist(tokens_before)

# Remove empty tokens  (caused by multiple spaces to be safe)
tokens_before <- tokens_before[tokens_before != ""]

# Count tokens: how many words are in the story (before cleaning)
numb_tokens_before <- length(tokens_before)

print (numb_tokens_before)


## Step 3: cleaning function (3+ chained procedures)
# Chained procedures included:
# 1) lowercase
# 2) replace tabs/newlines with spaces
# 3) remove punctuation (keep letters, numbers, apostrophes, spaces)
# 4) collapse multiple spaces
# 5) trim leading/trailing space

clean_text <- function(clean) {
  clean |>
    tolower() |>
    gsub("[\r\n\t]", " ", x = _) |>  # turn line breaks/newlines/tabs into spaces
    gsub("[^a-z0-9' ]+", " ", x = _) |> # remove punctuation except apostrophes and spaces
    gsub(" +", " ", x = _) |>          # collapse multiple spaces
    trimws()                          #remove leading/trailing whitespace
}


# Cleaning text using the above made function

cleaned_text <- clean_text(raw_text)


# Quick check: first 300 characters after cleaning

substr(cleaned_text, 1, 300)



## Step 4: Token counts: Split into one-word-per-row format (after cleaning, separator is " ")

tokens_after <- unlist(strsplit(cleaned_text, " ", fixed = TRUE))
tokens_after <- tokens_after[tokens_after != ""] # Remove empty tokens again just to be safe


# One-word-per-row data frame
one_word_per_row <- data.frame(word = tokens_after, stringsAsFactors = FALSE)

# Show the first 20 rows and becuse I prefer nice tables

knitr::kable(
  head(one_word_per_row, 20),
    caption = "One Word Per Row (After Cleaning)"
)


# count tokens: how many words are in the story (AFTER cleaning)
numb_tokens_after <- length(tokens_after)
print(numb_tokens_after)


## Step 5: summary table: before vs after
summary_table <-data.frame(
  measure = c("tokens_before_cleaning", "tokens_after_cleaning"),
  value   = c(numb_tokens_before, numb_tokens_after)
)

knitr::kable(summary_table, caption = "Token Count Before vs After Cleaning")


## Step 6: Type-Token Ratio (TTR) Before vs after
# TTR = number of unique tokens / total tokens

# Before cleaning
numb_tokens_before <- length(tokens_before)
numb_types_before  <- length(unique(tokens_before))
ttr_before      <- numb_types_before / numb_tokens_before

# After cleaning
numb_tokens_after <- length(tokens_after)
numb_types_after  <- length(unique(tokens_after))
ttr_after      <- numb_types_after / numb_tokens_after

# Results table for both before and after
ttr_results <- data.frame(
  version = c("before_cleaning", "after_cleaning"),
  tokens  = c(numb_tokens_before, numb_tokens_after),
  types   = c(numb_types_before, numb_types_after),
  TTR     = c(ttr_before, ttr_after)
)

print(ttr_results)

knitr::kable(ttr_results, caption = "Type-Token Ratio (TTR) Before vs After Cleaning")


## Optional- Step 7: Show three words per row for reability (After Cleaning)

total_tokens <- length(tokens_after)

padding_needed <- (3 - (total_tokens %% 3)) %% 3
tokens_padded <- c(tokens_after, rep(NA, padding_needed))

three_word_matrix <- matrix(tokens_padded, ncol = 3, byrow = TRUE)


# Convert matrix to a data frame so it prints very well
three_word_table <- as.data.frame(three_word_matrix)
colnames(three_word_table) <- c("Word 1", "Word 2", "Word 3")

# Show first 10 rows as a table in the knitted output
knitr::kable(
  head(three_word_table, 10),
  caption = "Three Words Per Row (After Cleaning)"
)


```
## Two cheat sheets

### Cheat Sheet 1: Splitting + Counting Tokens (5 things)

1. `strsplit(text, " ", fixed = TRUE)` splits on exactly one space.
2. `unlist()` turns the list output into a vector.
3. Remove empties with `tokens[tokens != ""]` if multiple spaces exist.
4. Count tokens with `length(tokens)`.
5. Count unique tokens with `length(unique(tokens))`.

### Cheat Sheet 2: Cleaning Basics

1. `tolower()` normalizes case.
2. `gsub("[\r\n\t]", " ", x)` converts newlines/tabs to spaces.
3. `gsub("[^a-z0-9' ]+", " ", x)` removes everything except letters, numbers, apostrophes, and spaces.
4. `gsub(" +", " ", x)` collapses multiple spaces into one.
5. `trimws()` removes leading/trailing spaces.
